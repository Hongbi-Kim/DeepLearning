{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiimWr21M6pE7etuk548m6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongbi-Kim/DeepLearning/blob/master/colab_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 텍스트의 토큰화"
      ],
      "metadata": {
        "id": "CCQNlKKcS3sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 주어진 문장을 '단어'로 토큰화 하기\n",
        "- p246~247"
      ],
      "metadata": {
        "id": "nZmOtayNS8nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리 함수 text_to_word_sequence() 호출\n",
        "# 주어진 문장을 '단어'로 토큰화 하기(tokenize)\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "XCvXQddwSuIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리할 텍스트 정하기\n",
        "text = '해보지 않으면 해낼 수 없다'  # 문장 >> 단어\n",
        "\n",
        "# 해당 텍스트 토큰화\n",
        "result = text_to_word_sequence(text)\n",
        "print('\\n원문:\\n', text)\n",
        "print('\\n토큰화:\\n' , result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIBpEZYWTMXS",
        "outputId": "da392260-a2f4-4f4a-b3b8-961625581f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "원문:\n",
            " 해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해보지', '않으면', '해낼', '수', '없다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리 함수 Tokenizer() 호출\n",
        "# 단어의 빈도 수 계산\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "wqMHAZfxTaRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 하려는 세개의 문장을 정하기\n",
        "\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']"
      ],
      "metadata": {
        "id": "C5ABpU3TTsQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 함수를 이용해 전처리하는 과정\n",
        "token = Tokenizer()       # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)  # 토큰화 함수에 문장 적용\n",
        "\n",
        "# 각 옵션에 맞춰 단어의 빈도 수를 계산한 결과 출력\n",
        "# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스 사용\n",
        "print('\\n단어 카운트:\\n', token.word_counts) \n",
        "\n",
        "# 출력되는 순서는 랜덤\n",
        "print('\\n문장 카운트:\\n', token.document_count)\n",
        "print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n",
        "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mekuQP3_Tu1V",
        "outputId": "d3cc1b13-c038-4e5a-ce3d-27fbc51646b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트:\n",
            " 3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'각': 1, '단어를': 1, '나누어': 1, '토큰화합니다': 1, '먼저': 1, '텍스트의': 2, '토큰화해야': 1, '단어로': 1, '딥러닝에서': 2, '인식됩니다': 1, '있습니다': 1, '사용할': 1, '결과는': 1, '수': 1, '토큰화한': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_P3c85l-YUGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 텍스트를 읽고 긍정, 부정 예측하기\n",
        "## 영화 리뷰가 긍정적인지 부정적인지 예측하기\n",
        "- p257, 258"
      ],
      "metadata": {
        "id": "Pl7qNK7RUVMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding"
      ],
      "metadata": {
        "id": "kPqESP_pUP1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 리뷰 자료 지정\n",
        "docs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다',\n",
        "        '한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요',\n",
        "        '연기가 어색해요','재미없어요']"
      ],
      "metadata": {
        "id": "1dc_AfnLU-s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성사전 : 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스 지정\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "fKQtpcUzVKvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5_I09pgVP65",
        "outputId": "2184ebb2-ff35-4d34-ca1c-dc2b87a981d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰에 지정된 인덱스로 새로운 배열을 생성\n",
        "x = token.texts_to_sequences(docs)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTVo0IVbVl4L",
        "outputId": "03949374-a49a-4149-d3d9-9add83db1706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩, 서로 다른 길이의 데이터를 지정된 값(여기서 4로 지정) 맞춤.\n",
        "padded_x = pad_sequences(x,4)\n",
        "print('\\n패딩 결과\\n', padded_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whbFhIH6VVwd",
        "outputId": "0f4e2137-f33c-4fab-d66c-d90d75133b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "패딩 결과\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 함수에 필요한 세 가지 파라미터 : 입력, 출력, 단어 수\n",
        "# 입력 : 총 몇 개의 단어 집합에서\n",
        "# 출력 : 몇 개의 임베딩 결과를 사용할 것인지\n",
        "# 단어 수 : 매번 입력될 단어 수는 몇 개로 할지\n",
        "\n",
        "# 딥러닝 모델 적용\n",
        "# 임베딩에 입력될 단어 수 지정\n",
        "# 전체 단어의 맨 앞에 0이 먼저 나와야 하므로 총 단어 수에 1을 더해줌\n",
        "word_size = len(token.word_index) + 1"
      ],
      "metadata": {
        "id": "RwUOLL3mVdv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩(word embedding)을 포함하여 딥러닝 모델을 만들고 결과 출력\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size, 8, input_length = 4))\n",
        "# word_size 만큼의 입력값, 8개의 임베딩 결과, input_length : 단어 수 지정\n",
        "model.add(Flatten()) # 차원축소(2차원 >> 1차원)\n",
        "model.add(Dense(1, activation = 'sigmoid')) # 출력층, 이진 분류(sigmoid)\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "model.fit(padded_x, classes, epochs = 20)\n",
        "\n",
        "print('\\n Accuracy: %.4f'%(model.evaluate(padded_x, classes)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABOTg0KTV3ya",
        "outputId": "fe08f8ef-1d30-4c04-aa34-343075a20cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.6915 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6898 - accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6882 - accuracy: 0.8000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6865 - accuracy: 0.8000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6849 - accuracy: 0.9000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6832 - accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6816 - accuracy: 0.9000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6799 - accuracy: 0.9000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6783 - accuracy: 0.9000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6766 - accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6749 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6733 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6716 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6699 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6682 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6665 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6647 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6630 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6612 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6595 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.6577 - accuracy: 1.0000\n",
            "\n",
            " Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 결과가 10개의 리뷰 샘플 중 몇 개를 맞추었는지 보여줌."
      ],
      "metadata": {
        "id": "mHE1ZBIhWUuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}